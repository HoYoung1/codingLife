"""
ì‹¤ì œ LLM ê¸°ë°˜ AI Agent ì˜ˆì œ

ì§„ì§œ ì¶”ë¡  ëŠ¥ë ¥ì„ ê°€ì§„ Agentê°€ ì–´ë–»ê²Œ ë™ì‘í•˜ëŠ”ì§€ ì‹œì—°í•©ë‹ˆë‹¤.
API í‚¤ê°€ ì—†ì„ ë•ŒëŠ” ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œë¡œ ë™ì‘í•˜ì—¬ ì‹¤ì œ LLMì˜ ë™ì‘ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
"""

import sys
import json
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from src.rag.rag_system import RAGSystem
from src.mcp.mcp_server import MCPServer, MCPTool
from src.mcp.tools.file_system_tools import FILE_SYSTEM_TOOLS
from src.mcp.tools.web_api_tools import WEB_API_TOOLS
from src.ai_agent.llm_agent import LLMAgent, LLMConfig, LLMProvider


def setup_systems():
    """RAG ì‹œìŠ¤í…œê³¼ MCP ì„œë²„ ì„¤ì •"""
    print("ğŸ”§ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
    
    # 1. RAG ì‹œìŠ¤í…œ ì„¤ì •
    print("ğŸ“š RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”...")
    rag = RAGSystem(use_ollama=False)
    
    # Agent ê°€ì´ë“œ ë¬¸ì„œ ìƒì„±
    docs_dir = Path("llm_agent_knowledge")
    docs_dir.mkdir(exist_ok=True)
    
    llm_agent_guide = """# LLM Agent ì‘ì—… ê°€ì´ë“œ

## íš¨ê³¼ì ì¸ ì‘ì—… ìˆ˜í–‰ ë°©ë²•

### 1. ìƒí™© ë¶„ì„
- ì‚¬ìš©ì ìš”ì²­ì„ ì •í™•íˆ ì´í•´í•˜ê¸°
- í˜„ì¬ ìƒí™©ê³¼ ëª©í‘œ ê°„ì˜ ì°¨ì´ íŒŒì•…
- í•„ìš”í•œ ì •ë³´ì™€ ë„êµ¬ ì‹ë³„

### 2. ê³„íš ìˆ˜ë¦½
- ëª©í‘œ ë‹¬ì„±ì„ ìœ„í•œ ë‹¨ê³„ë³„ ê³„íš
- ê° ë‹¨ê³„ì—ì„œ ì‚¬ìš©í•  ë„êµ¬ ì„ íƒ
- ì˜ˆìƒë˜ëŠ” ê²°ê³¼ì™€ ëŒ€ì•ˆ ê³„íš

### 3. ë„êµ¬ í™œìš© ê°€ì´ë“œ

#### íŒŒì¼ ì‹œìŠ¤í…œ ë„êµ¬ë“¤
- list_files: ë””ë ‰í† ë¦¬ íƒìƒ‰, íŒŒì¼ í˜„í™© íŒŒì•…
- read_file: íŒŒì¼ ë‚´ìš© í™•ì¸, ì •ë³´ ìˆ˜ì§‘
- write_file: ìƒˆ íŒŒì¼ ìƒì„±, ë‚´ìš© ì €ì¥
- create_directory: í´ë” êµ¬ì¡° ìƒì„±
- get_file_info: íŒŒì¼ ìƒì„¸ ì •ë³´ ì¡°íšŒ

#### ì •ë³´ ìˆ˜ì§‘ ë„êµ¬ë“¤
- get_weather: ë‚ ì”¨ ì •ë³´ ì¡°íšŒ
- get_news: ìµœì‹  ë‰´ìŠ¤ ìˆ˜ì§‘
- ì •ë³´ëŠ” ì‚¬ìš©ìì—ê²Œ ìœ ìš©í•œ í˜•íƒœë¡œ ê°€ê³µí•˜ì—¬ ì œê³µ

#### ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ë„êµ¬ë“¤
- send_slack_message: íŒ€ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜
- get_slack_channels: ì±„ë„ ì •ë³´ í™•ì¸

### 4. ê²°ê³¼ ë¶„ì„ ë° ê°œì„ 
- ê° ë‹¨ê³„ì˜ ì„±ê³µ/ì‹¤íŒ¨ ë¶„ì„
- ëª©í‘œ ë‹¬ì„±ë„ í‰ê°€
- í•„ìš”ì‹œ ê³„íš ìˆ˜ì • ë° ì¬ì‹¤í–‰

### 5. íš¨ê³¼ì ì¸ ì‘ì—… íŒ¨í„´
- ì •ë³´ ìˆ˜ì§‘ â†’ ë¶„ì„ â†’ í–‰ë™ â†’ ê²€ì¦ â†’ ê²°ê³¼ ì œê³µ
- ë‹¨ê³„ë³„ ì§„í–‰ ìƒí™© í™•ì¸
- ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” ê²°ê³¼ ë„ì¶œ
"""
    
    with open(docs_dir / "llm_agent_guide.txt", "w", encoding="utf-8") as f:
        f.write(llm_agent_guide)
    
    # RAGì— ë¬¸ì„œ ì¶”ê°€
    rag.add_documents([str(docs_dir / "llm_agent_guide.txt")])
    print("âœ… RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    # 2. MCP ì„œë²„ ì„¤ì •
    print("ğŸ”§ MCP ì„œë²„ ì´ˆê¸°í™”...")
    server = MCPServer("LLM Agent MCP ì„œë²„")
    
    # ëª¨ë“  ë„êµ¬ ë“±ë¡
    all_tools = FILE_SYSTEM_TOOLS + WEB_API_TOOLS
    for tool_config in all_tools:
        tool = MCPTool(
            name=tool_config["name"],
            description=tool_config["description"],
            parameters=tool_config["parameters"],
            function=tool_config["function"]
        )
        server.register_tool(tool)
    
    print(f"âœ… MCP ì„œë²„ ì´ˆê¸°í™” ì™„ë£Œ ({len(all_tools)}ê°œ ë„êµ¬)")
    
    return rag, server


def demonstrate_llm_vs_hardcoded():
    """LLM Agent vs í•˜ë“œì½”ë”© Agent ë¹„êµ ì‹œì—°"""
    print(f"\n{'='*80}")
    print("ğŸ§  LLM Agent vs ğŸ“‹ í•˜ë“œì½”ë”© Agent ë¹„êµ")
    print(f"{'='*80}")
    
    # ì‹œìŠ¤í…œ ì„¤ì •
    rag, mcp_server = setup_systems()
    
    # 1. í•˜ë“œì½”ë”© Agent (ê¸°ì¡´ êµ¬í˜„)
    print("\nğŸ“‹ í•˜ë“œì½”ë”© Agent ë™ì‘:")
    print("- ë¯¸ë¦¬ ì •ì˜ëœ if-else ë¡œì§")
    print("- ì˜ˆì¸¡ ê°€ëŠ¥í•œ ë™ì‘ íŒ¨í„´")
    print("- ì œí•œì ì¸ ìƒí™© ëŒ€ì‘")
    
    # 2. LLM Agent (ìƒˆë¡œìš´ êµ¬í˜„)
    print("\nğŸ§  LLM Agent ë™ì‘:")
    print("- ì‹¤ì‹œê°„ ìƒí™© ë¶„ì„ ë° ì¶”ë¡ ")
    print("- ë™ì  ê³„íš ìˆ˜ë¦½")
    print("- ì°½ì˜ì  ë¬¸ì œ í•´ê²°")
    
    # LLM Agent ìƒì„±
    llm_config = LLMConfig(
        provider=LLMProvider.SIMULATION,  # ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ
        model="gpt-3.5-turbo",
        temperature=0.7
    )
    
    llm_agent = LLMAgent(
        name="ì§€ëŠ¥í˜• Assistant",
        llm_config=llm_config,
        rag_system=rag,
        mcp_server=mcp_server
    )
    
    return llm_agent


def run_llm_agent_scenarios(agent: LLMAgent):
    """ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ë¡œ LLM Agent í…ŒìŠ¤íŠ¸"""
    
    scenarios = [
        {
            "name": "ë³µì¡í•œ ì •ë³´ ìˆ˜ì§‘ ì‘ì—…",
            "description": "í˜„ì¬ í”„ë¡œì íŠ¸ ìƒí™©ì„ íŒŒì•…í•˜ê³ , ì„œìš¸ ë‚ ì”¨ì™€ ìµœì‹  ê¸°ìˆ  ë‰´ìŠ¤ë¥¼ ì¡°í•©í•´ì„œ ì˜¤ëŠ˜ì˜ ì‘ì—… í™˜ê²½ ë¦¬í¬íŠ¸ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”",
            "expected_behavior": "íŒŒì¼ íƒìƒ‰ â†’ ë‚ ì”¨ ì¡°íšŒ â†’ ë‰´ìŠ¤ ìˆ˜ì§‘ â†’ ì¢…í•© ë¶„ì„ â†’ ë¦¬í¬íŠ¸ ìƒì„±"
        },
        {
            "name": "ì°½ì˜ì  ë¬¸ì œ í•´ê²°",
            "description": "íŒ€ í”„ë¡œì íŠ¸ ì§„í–‰ ìƒí™©ì„ ì •ë¦¬í•˜ê³  ë‹¤ìŒ ì£¼ ê³„íšì„ ì„¸ì›Œì£¼ì„¸ìš”",
            "expected_behavior": "í˜„ì¬ ìƒí™© ë¶„ì„ â†’ ì§„í–‰ ìƒí™© íŒŒì•… â†’ ê³„íš ìˆ˜ë¦½ â†’ ë¬¸ì„œí™”"
        },
        {
            "name": "ë‹¤ë‹¨ê³„ ì—…ë¬´ ì²˜ë¦¬",
            "description": "ê°œë°œ í™˜ê²½ì„ ì ê²€í•˜ê³  í•„ìš”í•œ ê°œì„ ì‚¬í•­ì„ ì°¾ì•„ì„œ ì•¡ì…˜ ì•„ì´í…œì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”",
            "expected_behavior": "í™˜ê²½ ì ê²€ â†’ ë¬¸ì œì  ì‹ë³„ â†’ ê°œì„ ì•ˆ ë„ì¶œ â†’ ì•¡ì…˜ í”Œëœ ì‘ì„±"
        }
    ]
    
    print(f"\n{'='*80}")
    print("ğŸ¯ LLM Agent ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸")
    print(f"{'='*80}")
    
    for i, scenario in enumerate(scenarios, 1):
        print(f"\nğŸ¬ ì‹œë‚˜ë¦¬ì˜¤ {i}: {scenario['name']}")
        print(f"ğŸ“‹ ìš”ì²­: {scenario['description']}")
        print(f"ğŸ¯ ì˜ˆìƒ ë™ì‘: {scenario['expected_behavior']}")
        print(f"{'-'*60}")
        
        # LLM Agent ì‹¤í–‰
        result = agent.execute_task(scenario['description'], max_steps=8)
        
        # ê²°ê³¼ ë¶„ì„
        print(f"\nğŸ“Š ì‹¤í–‰ ê²°ê³¼:")
        print(f"   ì„±ê³µ ì—¬ë¶€: {'âœ… ì„±ê³µ' if result['success'] else 'âŒ ì‹¤íŒ¨'}")
        print(f"   ì‹¤í–‰ ì‹œê°„: {result.get('execution_time', 0):.2f}ì´ˆ")
        print(f"   ì´ ë‹¨ê³„ ìˆ˜: {result.get('total_steps', 0)}")
        
        if result.get('error'):
            print(f"   ì˜¤ë¥˜: {result['error']}")
        
        # ì‹¤í–‰ ê³¼ì • ìƒì„¸ ë¶„ì„
        print(f"\nğŸ” ì‹¤í–‰ ê³¼ì • ë¶„ì„:")
        
        thoughts = [h for h in result.get('history', []) if h['type'] == 'thought']
        actions = [h for h in result.get('history', []) if h['type'] == 'action']
        observations = [h for h in result.get('history', []) if h['type'] == 'observation']
        
        print(f"   ğŸ§  ì‚¬ê³  ê³¼ì •: {len(thoughts)}íšŒ")
        print(f"   ğŸ¬ í–‰ë™ ì‹¤í–‰: {len(actions)}íšŒ")
        print(f"   ğŸ‘ï¸  ê²°ê³¼ ê´€ì°°: {len(observations)}íšŒ")
        
        # ì£¼ìš” ì‚¬ê³  ê³¼ì • ì¶œë ¥
        if thoughts:
            print(f"\nğŸ’­ ì£¼ìš” ì‚¬ê³  ê³¼ì •:")
            for j, thought in enumerate(thoughts[:3], 1):
                print(f"   {j}. {thought['content'][:150]}...")
        
        # ì‹¤í–‰í•œ ë„êµ¬ë“¤
        if actions:
            print(f"\nğŸ› ï¸  ì‚¬ìš©í•œ ë„êµ¬ë“¤:")
            for action in actions:
                if isinstance(action['content'], dict) and 'tool_name' in action['content']:
                    tool_name = action['content']['tool_name']
                    success = "âœ…" if action['content'].get('success') else "âŒ"
                    print(f"   {success} {tool_name}")
        
        # ìµœì¢… ìš”ì•½
        if result.get('final_summary'):
            print(f"\nğŸ“ ìµœì¢… ìš”ì•½:")
            print(f"   {result['final_summary'][:200]}...")
        
        print(f"\nâ±ï¸  ì‹œë‚˜ë¦¬ì˜¤ {i} ì™„ë£Œ\n")


def interactive_llm_agent():
    """ëŒ€í™”í˜• LLM Agent"""
    print(f"\n{'='*80}")
    print("ğŸ’¬ ëŒ€í™”í˜• LLM Agent")
    print(f"{'='*80}")
    
    # ì‹œìŠ¤í…œ ì„¤ì •
    rag, mcp_server = setup_systems()
    
    # LLM Agent ìƒì„±
    llm_config = LLMConfig(
        provider=LLMProvider.SIMULATION,
        model="gpt-3.5-turbo",
        temperature=0.7
    )
    
    agent = LLMAgent(
        name="ëŒ€í™”í˜• ì§€ëŠ¥ Assistant",
        llm_config=llm_config,
        rag_system=rag,
        mcp_server=mcp_server
    )
    
    print("\nğŸ¤– ì§€ëŠ¥í˜• Agentì™€ ëŒ€í™”í•´ë³´ì„¸ìš”!")
    print("ğŸ’¡ ì´ AgentëŠ” ì‹¤ì œ LLMì²˜ëŸ¼ ìƒí™©ì„ ë¶„ì„í•˜ê³  ê³„íšì„ ì„¸ì›ë‹ˆë‹¤.")
    print("\nì˜ˆì‹œ ìš”ì²­:")
    print("- 'í”„ë¡œì íŠ¸ í˜„í™©ì„ íŒŒì•…í•˜ê³  ì£¼ê°„ ë¦¬í¬íŠ¸ë¥¼ ë§Œë“¤ì–´ì¤˜'")
    print("- 'ë‚ ì”¨ í™•ì¸í•˜ê³  ì˜¤ëŠ˜ ì¼ì •ì— ë§ëŠ” ì¡°ì–¸ì„ í•´ì¤˜'")
    print("- 'ê°œë°œ í™˜ê²½ì„ ì ê²€í•˜ê³  ê°œì„ ì‚¬í•­ì„ ì°¾ì•„ì¤˜'")
    print("- 'íŒ€ì—ê²Œ ê³µìœ í•  ê¸°ìˆ  ë‰´ìŠ¤ ë¸Œë¦¬í•‘ì„ ë§Œë“¤ì–´ì¤˜'")
    print("\nì¢…ë£Œí•˜ë ¤ë©´ 'quit' ì…ë ¥")
    
    while True:
        try:
            user_input = input(f"\nğŸ‘¤ ì‚¬ìš©ì: ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ', 'q']:
                print("ğŸ‘‹ ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            if not user_input:
                continue
            
            print(f"\nğŸ§  Agentê°€ ìš”ì²­ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
            
            # LLM Agent ì‹¤í–‰
            result = agent.execute_task(user_input, max_steps=6)
            
            # ê²°ê³¼ ì¶œë ¥
            if result['success']:
                print(f"\nâœ… ì‘ì—… ì™„ë£Œ!")
                
                # ìµœì¢… ìš”ì•½ ì¶œë ¥
                if result.get('final_summary'):
                    print(f"\nğŸ“‹ ê²°ê³¼ ìš”ì•½:")
                    print(f"{result['final_summary']}")
                
                # ì‹¤í–‰ ê³¼ì • ê°„ë‹¨íˆ ì¶œë ¥
                thoughts = [h for h in result.get('history', []) if h['type'] == 'thought']
                actions = [h for h in result.get('history', []) if h['type'] == 'action']
                
                print(f"\nğŸ” ì‹¤í–‰ ê³¼ì •: {len(thoughts)}ë‹¨ê³„ ì‚¬ê³ , {len(actions)}ê°œ ë„êµ¬ ì‚¬ìš©")
                
            else:
                print(f"\nâŒ ì‘ì—… ì²˜ë¦¬ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
                if result.get('error'):
                    print(f"ì˜¤ë¥˜: {result['error']}")
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=== ì‹¤ì œ LLM ê¸°ë°˜ AI Agent ì˜ˆì œ ===")
    print("ğŸ§  ì§„ì§œ ì¶”ë¡  ëŠ¥ë ¥ì„ ê°€ì§„ Agentì˜ ë™ì‘ì„ ì‹œì—°í•©ë‹ˆë‹¤.")
    print("ğŸ­ API í‚¤ê°€ ì—†ìœ¼ë¯€ë¡œ ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œë¡œ ì‹¤ì œ LLM ë™ì‘ì„ ëª¨ë°©í•©ë‹ˆë‹¤.\n")
    
    # LLM vs í•˜ë“œì½”ë”© ë¹„êµ
    llm_agent = demonstrate_llm_vs_hardcoded()
    
    # ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸
    run_llm_agent_scenarios(llm_agent)
    
    # ëŒ€í™”í˜• ë°ëª¨ ì„ íƒ
    print(f"\n{'='*80}")
    choice = input("ëŒ€í™”í˜• LLM Agentë¥¼ ì²´í—˜í•´ë³´ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").strip().lower()
    
    if choice in ['y', 'yes', 'ì˜ˆ', 'ã…‡']:
        interactive_llm_agent()
    
    print(f"\nğŸ‰ LLM Agent ì˜ˆì œ ì™„ë£Œ!")
    
    print(f"\nğŸ’¡ í•µì‹¬ ì°¨ì´ì :")
    print("ğŸ“‹ í•˜ë“œì½”ë”© Agent:")
    print("   - ifë¬¸ìœ¼ë¡œ ê³ ì •ëœ ë¡œì§")
    print("   - ì˜ˆì¸¡ ê°€ëŠ¥í•œ ë™ì‘")
    print("   - ìƒˆë¡œìš´ ìƒí™©ì— ëŒ€ì‘ ì–´ë ¤ì›€")
    
    print("\nğŸ§  LLM Agent:")
    print("   - ì‹¤ì‹œê°„ ìƒí™© ë¶„ì„ ë° ì¶”ë¡ ")
    print("   - ë™ì  ê³„íš ìˆ˜ë¦½")
    print("   - ì°½ì˜ì  ë¬¸ì œ í•´ê²°")
    print("   - ìì—°ì–´ë¡œ ì‚¬ê³  ê³¼ì • ì„¤ëª…")
    
    print(f"\nğŸš€ ì‹¤ì œ êµ¬í˜„ ì‹œ:")
    print("   - OpenAI API: config.provider = LLMProvider.OPENAI")
    print("   - Ollama: config.provider = LLMProvider.OLLAMA")
    print("   - API í‚¤ë§Œ ì„¤ì •í•˜ë©´ ì§„ì§œ LLM ì—°ë™!")
    
    print(f"\nğŸ¯ ì´ì œ ì§„ì§œ ì§€ëŠ¥ì ì¸ Agentì˜ êµ¬ì¡°ë¥¼ ì´í•´í–ˆìŠµë‹ˆë‹¤!")


if __name__ == "__main__":
    main()