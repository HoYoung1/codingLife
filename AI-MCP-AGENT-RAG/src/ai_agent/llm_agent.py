"""
ì‹¤ì œ LLM ê¸°ë°˜ AI Agent êµ¬í˜„

OpenAI API ë˜ëŠ” ë‹¤ë¥¸ LLMì„ ì‚¬ìš©í•˜ì—¬ ì§„ì§œ ì¶”ë¡  ëŠ¥ë ¥ì„ ê°€ì§„ Agentë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.
API í‚¤ê°€ ì—†ì„ ë•ŒëŠ” ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤.
"""

import json
import logging
import time
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from enum import Enum

# ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì´ëŸ° ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ ì‚¬ìš©
try:
    import openai  # pip install openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

try:
    from langchain.llms import Ollama  # ë¡œì»¬ LLM
    OLLAMA_AVAILABLE = True
except ImportError:
    OLLAMA_AVAILABLE = False

logger = logging.getLogger(__name__)


class LLMProvider(Enum):
    """LLM ì œê³µì"""
    OPENAI = "openai"
    OLLAMA = "ollama"
    SIMULATION = "simulation"


@dataclass
class LLMConfig:
    """LLM ì„¤ì •"""
    provider: LLMProvider
    model: str = "gpt-3.5-turbo"
    api_key: Optional[str] = None
    base_url: Optional[str] = None
    temperature: float = 0.7
    max_tokens: int = 1000


class LLMInterface:
    """
    LLM ì¸í„°í˜ì´ìŠ¤ - ë‹¤ì–‘í•œ LLM ì œê³µìë¥¼ í†µí•©
    
    ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” OpenAI, Ollama ë“±ì„ ì‚¬ìš©í•˜ê³ ,
    API í‚¤ê°€ ì—†ì„ ë•ŒëŠ” ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œë¡œ ë™ì‘
    """
    
    def __init__(self, config: LLMConfig):
        self.config = config
        self.client = None
        self._setup_client()
    
    def _setup_client(self):
        """LLM í´ë¼ì´ì–¸íŠ¸ ì„¤ì •"""
        try:
            if self.config.provider == LLMProvider.OPENAI:
                if OPENAI_AVAILABLE and self.config.api_key:
                    # ì‹¤ì œ OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
                    openai.api_key = self.config.api_key
                    self.client = "openai"
                    logger.info("âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì • ì™„ë£Œ")
                else:
                    logger.warning("âš ï¸  OpenAI API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                    self.config.provider = LLMProvider.SIMULATION
            
            elif self.config.provider == LLMProvider.OLLAMA:
                if OLLAMA_AVAILABLE:
                    # ì‹¤ì œ Ollama í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
                    self.client = Ollama(model=self.config.model)
                    logger.info(f"âœ… Ollama í´ë¼ì´ì–¸íŠ¸ ì„¤ì • ì™„ë£Œ: {self.config.model}")
                else:
                    logger.warning("âš ï¸  Ollamaê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                    self.config.provider = LLMProvider.SIMULATION
            
            if self.config.provider == LLMProvider.SIMULATION:
                self.client = "simulation"
                logger.info("ğŸ­ ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œë¡œ LLM ë™ì‘ì„ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤.")
                
        except Exception as e:
            logger.error(f"LLM í´ë¼ì´ì–¸íŠ¸ ì„¤ì • ì‹¤íŒ¨: {str(e)}")
            self.config.provider = LLMProvider.SIMULATION
            self.client = "simulation"
    
    def generate(self, prompt: str, system_prompt: str = "") -> str:
        """í…ìŠ¤íŠ¸ ìƒì„±"""
        try:
            if self.config.provider == LLMProvider.OPENAI:
                return self._generate_openai(prompt, system_prompt)
            elif self.config.provider == LLMProvider.OLLAMA:
                return self._generate_ollama(prompt, system_prompt)
            else:
                return self._generate_simulation(prompt, system_prompt)
                
        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return self._generate_simulation(prompt, system_prompt)
    
    def _generate_openai(self, prompt: str, system_prompt: str = "") -> str:
        """OpenAI API í˜¸ì¶œ"""
        # ì‹¤ì œ êµ¬í˜„ ì½”ë“œ
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        # ì‹¤ì œë¡œëŠ” ì´ë ‡ê²Œ í˜¸ì¶œ
        # response = openai.ChatCompletion.create(
        #     model=self.config.model,
        #     messages=messages,
        #     temperature=self.config.temperature,
        #     max_tokens=self.config.max_tokens
        # )
        # return response.choices[0].message.content
        
        # API í‚¤ê°€ ì—†ìœ¼ë¯€ë¡œ ì‹œë®¬ë ˆì´ì…˜ìœ¼ë¡œ ëŒ€ì²´
        return self._generate_simulation(prompt, system_prompt)
    
    def _generate_ollama(self, prompt: str, system_prompt: str = "") -> str:
        """Ollama í˜¸ì¶œ"""
        full_prompt = f"{system_prompt}\n\n{prompt}" if system_prompt else prompt
        
        # ì‹¤ì œë¡œëŠ” ì´ë ‡ê²Œ í˜¸ì¶œ
        # response = self.client(full_prompt)
        # return response
        
        # Ollamaê°€ ì—†ìœ¼ë¯€ë¡œ ì‹œë®¬ë ˆì´ì…˜ìœ¼ë¡œ ëŒ€ì²´
        return self._generate_simulation(prompt, system_prompt)
    
    def _generate_simulation(self, prompt: str, system_prompt: str = "") -> str:
        """
        ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ - ì‹¤ì œ LLMì²˜ëŸ¼ ë™ì‘í•˜ëŠ” ê°€ì§œ ì‘ë‹µ ìƒì„±
        
        ì‹¤ì œ LLMì˜ ì¶”ë¡  íŒ¨í„´ì„ ëª¨ë°©í•˜ì—¬ ìƒí™©ì— ë§ëŠ” ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
        """
        # ì•½ê°„ì˜ ì§€ì—° ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œ API í˜¸ì¶œì²˜ëŸ¼)
        time.sleep(0.5)
        
        # í”„ë¡¬í”„íŠ¸ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ì‘ë‹µ ìƒì„±
        prompt_lower = prompt.lower()
        
        # ì‚¬ê³  ê³¼ì • ìš”ì²­
        if "ìƒê°" in prompt or "ë¶„ì„" in prompt or "think" in prompt_lower:
            if "íŒŒì¼" in prompt or "file" in prompt_lower:
                return """í˜„ì¬ ìƒí™©ì„ ë¶„ì„í•´ë³´ê² ìŠµë‹ˆë‹¤.

ì‚¬ìš©ìê°€ íŒŒì¼ ê´€ë ¨ ì‘ì—…ì„ ìš”ì²­í–ˆìŠµë‹ˆë‹¤. ë¨¼ì € í˜„ì¬ ë””ë ‰í† ë¦¬ì˜ íŒŒì¼ ëª©ë¡ì„ í™•ì¸í•˜ì—¬ ì–´ë–¤ íŒŒì¼ë“¤ì´ ìˆëŠ”ì§€ íŒŒì•…í•´ì•¼ í•©ë‹ˆë‹¤. ê·¸ ë‹¤ìŒì— ì‚¬ìš©ìì˜ êµ¬ì²´ì ì¸ ìš”êµ¬ì‚¬í•­ì— ë”°ë¼ íŒŒì¼ì„ ì½ê±°ë‚˜, ìƒì„±í•˜ê±°ë‚˜, ìˆ˜ì •í•˜ëŠ” ì‘ì—…ì„ ìˆ˜í–‰í•˜ê² ìŠµë‹ˆë‹¤.

ë‹¤ìŒ ë‹¨ê³„: list_files ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í˜„ì¬ ë””ë ‰í† ë¦¬ íƒìƒ‰"""

            elif "ë‚ ì”¨" in prompt or "weather" in prompt_lower:
                return """ì‚¬ìš©ìê°€ ë‚ ì”¨ ì •ë³´ë¥¼ ìš”ì²­í–ˆìŠµë‹ˆë‹¤.

ë‚ ì”¨ ì •ë³´ë¥¼ ì œê³µí•˜ê¸° ìœ„í•´ì„œëŠ” get_weather ë„êµ¬ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤. ì‚¬ìš©ìê°€ íŠ¹ì • ë„ì‹œë¥¼ ì–¸ê¸‰í•˜ì§€ ì•Šì•˜ë‹¤ë©´ ê¸°ë³¸ì ìœ¼ë¡œ ì„œìš¸ì˜ ë‚ ì”¨ë¥¼ ì¡°íšŒí•˜ê² ìŠµë‹ˆë‹¤. ë‚ ì”¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜¨ í›„ì—ëŠ” ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ ì •ë¦¬í•´ì„œ ì œê³µí•˜ê² ìŠµë‹ˆë‹¤.

ë‹¤ìŒ ë‹¨ê³„: get_weather ë„êµ¬ë¡œ ë‚ ì”¨ ì •ë³´ ì¡°íšŒ"""

            elif "ë‰´ìŠ¤" in prompt or "news" in prompt_lower:
                return """ì‚¬ìš©ìê°€ ë‰´ìŠ¤ ì •ë³´ë¥¼ ìš”ì²­í–ˆìŠµë‹ˆë‹¤.

ìµœì‹  ë‰´ìŠ¤ë¥¼ ì œê³µí•˜ê¸° ìœ„í•´ get_news ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤. í•œêµ­ ë‰´ìŠ¤ë¥¼ ê¸°ë³¸ìœ¼ë¡œ í•˜ë˜, ê¸°ìˆ  ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ê°€ì ¸ì™€ì„œ ì‚¬ìš©ìì—ê²Œ ìœ ìš©í•œ ì •ë³´ë¥¼ ì œê³µí•˜ê² ìŠµë‹ˆë‹¤.

ë‹¤ìŒ ë‹¨ê³„: get_news ë„êµ¬ë¡œ ìµœì‹  ë‰´ìŠ¤ ì¡°íšŒ"""

            else:
                return """í˜„ì¬ ìƒí™©ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤.

ì‚¬ìš©ìì˜ ìš”ì²­ì„ ì´í•´í•˜ê³  ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•œ ìµœì ì˜ ë°©ë²•ì„ ì°¾ê³  ìˆìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ë“¤ì„ ê²€í† í•˜ì—¬ ê°€ì¥ ì ì ˆí•œ ë„êµ¬ë¥¼ ì„ íƒí•˜ê² ìŠµë‹ˆë‹¤.

ë‹¤ìŒ ë‹¨ê³„: ì ì ˆí•œ ë„êµ¬ ì„ íƒ ë° ì‹¤í–‰"""

        # ë„êµ¬ ì„ íƒ ìš”ì²­
        elif "ë„êµ¬" in prompt or "tool" in prompt_lower or "action" in prompt_lower:
            if "íŒŒì¼" in prompt:
                return """{
    "tool_name": "list_files",
    "parameters": {
        "directory": ".",
        "show_hidden": false
    },
    "reasoning": "í˜„ì¬ ë””ë ‰í† ë¦¬ì˜ íŒŒì¼ ëª©ë¡ì„ í™•ì¸í•˜ì—¬ ì‚¬ìš©ìê°€ ì‘ì—…í•  ìˆ˜ ìˆëŠ” íŒŒì¼ë“¤ì„ íŒŒì•…í•©ë‹ˆë‹¤."
}"""
            elif "ë‚ ì”¨" in prompt:
                return """{
    "tool_name": "get_weather",
    "parameters": {
        "city": "ì„œìš¸",
        "country": "KR",
        "units": "metric"
    },
    "reasoning": "ì„œìš¸ì˜ í˜„ì¬ ë‚ ì”¨ ì •ë³´ë¥¼ ì¡°íšŒí•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ì œê³µí•©ë‹ˆë‹¤."
}"""
            elif "ë‰´ìŠ¤" in prompt:
                return """{
    "tool_name": "get_news",
    "parameters": {
        "country": "kr",
        "category": "technology",
        "max_articles": 5
    },
    "reasoning": "í•œêµ­ì˜ ìµœì‹  ê¸°ìˆ  ë‰´ìŠ¤ë¥¼ ì¡°íšŒí•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ìœ ìš©í•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤."
}"""
            else:
                return """{
    "tool_name": "list_files",
    "parameters": {
        "directory": "."
    },
    "reasoning": "ê¸°ë³¸ì ìœ¼ë¡œ í˜„ì¬ ìƒí™©ì„ íŒŒì•…í•˜ê¸° ìœ„í•´ íŒŒì¼ ëª©ë¡ì„ í™•ì¸í•©ë‹ˆë‹¤."
}"""

        # ê²°ê³¼ ë¶„ì„ ìš”ì²­
        elif "ê²°ê³¼" in prompt or "ê´€ì°°" in prompt or "observe" in prompt_lower:
            if "ì„±ê³µ" in prompt or "success" in prompt_lower:
                return """ë„êµ¬ ì‹¤í–‰ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.

ê²°ê³¼ë¥¼ ë¶„ì„í•´ë³´ë‹ˆ ì‚¬ìš©ìì˜ ìš”ì²­ì— í•„ìš”í•œ ì •ë³´ë¥¼ ì„±ê³µì ìœ¼ë¡œ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤. ì´ì œ ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ì§„í–‰í•˜ê±°ë‚˜, ì‚¬ìš©ìì—ê²Œ ê²°ê³¼ë¥¼ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ëª©í‘œ ë‹¬ì„± ì—¬ë¶€: ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ, ì¶”ê°€ ì‘ì—… í•„ìš” ì—¬ë¶€ ê²€í†  ì¤‘"""

            else:
                return """ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤.

ì‹¤í–‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê³„íší•˜ê³  ìˆìŠµë‹ˆë‹¤. í•„ìš”í•˜ë‹¤ë©´ ì¶”ê°€ì ì¸ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜, í˜„ì¬ê¹Œì§€ì˜ ê²°ê³¼ë¥¼ ì •ë¦¬í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ì œê³µí•˜ê² ìŠµë‹ˆë‹¤."""

        # ê³„íš ìˆ˜ë¦½ ìš”ì²­
        elif "ê³„íš" in prompt or "plan" in prompt_lower:
            return """ì‘ì—… ê³„íšì„ ìˆ˜ë¦½í•˜ê² ìŠµë‹ˆë‹¤.

1ë‹¨ê³„: í˜„ì¬ ìƒí™© íŒŒì•… ë° í•„ìš”í•œ ì •ë³´ ìˆ˜ì§‘
2ë‹¨ê³„: ì‚¬ìš©ì ìš”ì²­ì— ë§ëŠ” ì ì ˆí•œ ë„êµ¬ ì„ íƒ
3ë‹¨ê³„: ë„êµ¬ ì‹¤í–‰ ë° ê²°ê³¼ í™•ì¸
4ë‹¨ê³„: ê²°ê³¼ ë¶„ì„ ë° ì¶”ê°€ ì‘ì—… í•„ìš”ì„± íŒë‹¨
5ë‹¨ê³„: ìµœì¢… ê²°ê³¼ ì •ë¦¬ ë° ì‚¬ìš©ìì—ê²Œ ì œê³µ

ì´ ê³„íšì— ë”°ë¼ ë‹¨ê³„ë³„ë¡œ ì‘ì—…ì„ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤."""

        # ê¸°ë³¸ ì‘ë‹µ
        else:
            return """ë„¤, ì´í•´í–ˆìŠµë‹ˆë‹¤. 

ì‚¬ìš©ìì˜ ìš”ì²­ì„ ë¶„ì„í•˜ê³  ìµœì ì˜ ë°©ë²•ìœ¼ë¡œ ì²˜ë¦¬í•˜ê² ìŠµë‹ˆë‹¤. í•„ìš”í•œ ë„êµ¬ë“¤ì„ í™œìš©í•˜ì—¬ ì •í™•í•˜ê³  ìœ ìš©í•œ ê²°ê³¼ë¥¼ ì œê³µí•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤."""


class LLMAgent:
    """
    ì‹¤ì œ LLM ê¸°ë°˜ AI Agent
    
    ì§„ì§œ ì¶”ë¡  ëŠ¥ë ¥ì„ ê°€ì§„ Agentë¡œ, ìƒí™©ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ê³„íšì„ ì„¸ìš°ê³ 
    ì ì ˆí•œ ë„êµ¬ë¥¼ ì„ íƒí•˜ì—¬ ëª©í‘œë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, name: str, llm_config: LLMConfig, rag_system=None, mcp_server=None):
        self.name = name
        self.llm = LLMInterface(llm_config)
        self.rag_system = rag_system
        self.mcp_server = mcp_server
        
        self.available_tools: List[Dict] = []
        self.execution_history: List[Dict] = []
        self.current_task: Optional[Dict] = None
        
        # MCP ë„êµ¬ ëª©ë¡ ë¡œë“œ
        if self.mcp_server:
            self._load_available_tools()
        
        logger.info(f"ğŸ§  LLM ê¸°ë°˜ AI Agent '{name}' ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _load_available_tools(self):
        """MCP ì„œë²„ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ ë¡œë“œ"""
        try:
            request = '{"action": "list_tools"}'
            response = self.mcp_server.handle_request(request)
            response_data = json.loads(response)
            
            if response_data["success"]:
                self.available_tools = response_data["tools"]
                logger.info(f"âœ… {len(self.available_tools)}ê°œ ë„êµ¬ ë¡œë“œ ì™„ë£Œ")
            else:
                logger.error(f"ë„êµ¬ ë¡œë“œ ì‹¤íŒ¨: {response_data.get('error')}")
                
        except Exception as e:
            logger.error(f"ë„êµ¬ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    def execute_task(self, task_description: str, max_steps: int = 10) -> Dict[str, Any]:
        """
        LLM ê¸°ë°˜ ì‘ì—… ì‹¤í–‰
        
        ì‹¤ì œ LLMì´ ìƒí™©ì„ ë¶„ì„í•˜ê³ , ê³„íšì„ ì„¸ìš°ê³ , ë„êµ¬ë¥¼ ì„ íƒí•˜ì—¬ ì‹¤í–‰í•©ë‹ˆë‹¤.
        """
        self.current_task = {
            "description": task_description,
            "start_time": time.time(),
            "max_steps": max_steps
        }
        self.execution_history = []
        
        logger.info(f"ğŸš€ LLM Agent ì‘ì—… ì‹œì‘: {task_description}")
        
        try:
            step_number = 1
            
            while step_number <= max_steps:
                logger.info(f"ğŸ“ Step {step_number}: ì‚¬ê³  ê³¼ì • ì‹œì‘")
                
                # 1. Think: LLMì´ í˜„ì¬ ìƒí™©ì„ ë¶„ì„í•˜ê³  ë‹¤ìŒ í–‰ë™ì„ ê³„íš
                thought = self._llm_think(step_number)
                self._add_to_history("thought", thought, step_number)
                
                # ëª©í‘œ ë‹¬ì„± í™•ì¸
                if self._is_goal_achieved(thought):
                    logger.info("ğŸ¯ ëª©í‘œ ë‹¬ì„± ì™„ë£Œ!")
                    break
                
                # 2. Act: LLMì´ ë„êµ¬ë¥¼ ì„ íƒí•˜ê³  ì‹¤í–‰
                logger.info(f"ğŸ¬ Step {step_number}: í–‰ë™ ì‹¤í–‰")
                action_result = self._llm_act(thought, step_number)
                self._add_to_history("action", action_result, step_number)
                
                # 3. Observe: LLMì´ ê²°ê³¼ë¥¼ ê´€ì°°í•˜ê³  ë¶„ì„
                logger.info(f"ğŸ‘ï¸  Step {step_number}: ê²°ê³¼ ê´€ì°°")
                observation = self._llm_observe(action_result, step_number)
                self._add_to_history("observation", observation, step_number)
                
                step_number += 1
            
            execution_time = time.time() - self.current_task["start_time"]
            
            return {
                "success": True,
                "task": task_description,
                "execution_time": execution_time,
                "total_steps": len([h for h in self.execution_history if h["type"] == "thought"]),
                "history": self.execution_history,
                "final_summary": self._generate_final_summary()
            }
            
        except Exception as e:
            logger.error(f"ì‘ì—… ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "task": task_description,
                "history": self.execution_history
            }
    
    def _llm_think(self, step_number: int) -> str:
        """LLMì´ í˜„ì¬ ìƒí™©ì„ ë¶„ì„í•˜ê³  ë‹¤ìŒ í–‰ë™ì„ ê³„íš"""
        
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = self._build_context(step_number)
        
        # RAG ì‹œìŠ¤í…œìœ¼ë¡œ ê´€ë ¨ ì§€ì‹ ê²€ìƒ‰ (ìˆëŠ” ê²½ìš°)
        relevant_knowledge = ""
        if self.rag_system:
            try:
                rag_result = self.rag_system.query(self.current_task["description"])
                if rag_result.get("answer"):
                    relevant_knowledge = f"\nê´€ë ¨ ì§€ì‹:\n{rag_result['answer'][:300]}..."
            except Exception as e:
                logger.warning(f"RAG ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
        
        # LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        system_prompt = """ë‹¹ì‹ ì€ ììœ¨ì ìœ¼ë¡œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” AI Agentì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ë‹¨ê³„ë³„ë¡œ ìƒê°í•˜ê³  ì ì ˆí•œ ë„êµ¬ë¥¼ ì„ íƒí•˜ì—¬ ì‹¤í–‰í•©ë‹ˆë‹¤.
í˜„ì¬ ìƒí™©ì„ ë¶„ì„í•˜ê³  ë‹¤ìŒì— ë¬´ì—‡ì„ í•´ì•¼ í• ì§€ ëª…í™•í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”."""

        user_prompt = f"""
í˜„ì¬ ì‘ì—…: {self.current_task['description']}
ë‹¨ê³„: {step_number}/{self.current_task['max_steps']}

{context}

ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ë“¤:
{self._format_available_tools()}

{relevant_knowledge}

í˜„ì¬ ìƒí™©ì„ ë¶„ì„í•˜ê³  ë‹¤ìŒì— ë¬´ì—‡ì„ í•´ì•¼ í• ì§€ ìƒê°í•´ë³´ì„¸ìš”.
ëª©í‘œ ë‹¬ì„±ì„ ìœ„í•œ êµ¬ì²´ì ì¸ ê³„íšì„ ì„¤ëª…í•˜ì„¸ìš”.
"""
        
        # LLM í˜¸ì¶œ
        thought = self.llm.generate(user_prompt, system_prompt)
        logger.info(f"ğŸ§  LLM ì‚¬ê³ : {thought[:100]}...")
        
        return thought
    
    def _llm_act(self, thought: str, step_number: int) -> Dict[str, Any]:
        """LLMì´ ë„êµ¬ë¥¼ ì„ íƒí•˜ê³  ì‹¤í–‰"""
        
        # LLMì—ê²Œ ë„êµ¬ ì„ íƒ ìš”ì²­
        system_prompt = """ë‹¹ì‹ ì€ ìƒê°í•œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì ì ˆí•œ ë„êµ¬ë¥¼ ì„ íƒí•˜ê³  ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤.
JSON í˜•íƒœë¡œ ë„êµ¬ ì´ë¦„ê³¼ íŒŒë¼ë¯¸í„°ë¥¼ ì •í™•íˆ ì§€ì •í•˜ì„¸ìš”."""

        user_prompt = f"""
í˜„ì¬ ìƒê°: {thought}

ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ë“¤:
{self._format_available_tools()}

ìœ„ì˜ ìƒê°ì„ ë°”íƒ•ìœ¼ë¡œ ì–´ë–¤ ë„êµ¬ë¥¼ ì–´ë–¤ íŒŒë¼ë¯¸í„°ë¡œ ì‹¤í–‰í•´ì•¼ í• ê¹Œìš”?

ë‹¤ìŒ JSON í˜•íƒœë¡œ ë‹µë³€í•˜ì„¸ìš”:
{{
    "tool_name": "ë„êµ¬_ì´ë¦„",
    "parameters": {{
        "param1": "value1",
        "param2": "value2"
    }},
    "reasoning": "ì´ ë„êµ¬ë¥¼ ì„ íƒí•œ ì´ìœ "
}}
"""
        
        # LLM í˜¸ì¶œ
        llm_response = self.llm.generate(user_prompt, system_prompt)
        
        try:
            # JSON íŒŒì‹± ì‹œë„
            if "{" in llm_response and "}" in llm_response:
                json_start = llm_response.find("{")
                json_end = llm_response.rfind("}") + 1
                json_str = llm_response[json_start:json_end]
                action_plan = json.loads(json_str)
            else:
                # JSONì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ë„êµ¬ ì„ íƒ
                action_plan = {
                    "tool_name": self.available_tools[0]["name"] if self.available_tools else "list_files",
                    "parameters": {},
                    "reasoning": "ê¸°ë³¸ ë„êµ¬ ì„ íƒ"
                }
            
            # ë„êµ¬ ì‹¤í–‰
            result = self._execute_tool(
                action_plan["tool_name"],
                action_plan.get("parameters", {}),
                step_number
            )
            
            result["reasoning"] = action_plan.get("reasoning", "")
            result["llm_response"] = llm_response
            
            return result
            
        except Exception as e:
            logger.error(f"ë„êµ¬ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "llm_response": llm_response
            }
    
    def _llm_observe(self, action_result: Dict[str, Any], step_number: int) -> str:
        """LLMì´ ì‹¤í–‰ ê²°ê³¼ë¥¼ ê´€ì°°í•˜ê³  ë¶„ì„"""
        
        system_prompt = """ë‹¹ì‹ ì€ ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê³  ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê³„íší•˜ëŠ” AI Agentì…ë‹ˆë‹¤.
ì‹¤í–‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëª©í‘œ ë‹¬ì„± ì—¬ë¶€ì™€ ë‹¤ìŒì— í•´ì•¼ í•  ì¼ì„ ë¶„ì„í•˜ì„¸ìš”."""

        user_prompt = f"""
ì‹¤í–‰í•œ ë„êµ¬: {action_result.get('tool_name', 'unknown')}
ì‹¤í–‰ ê²°ê³¼: {json.dumps(action_result, ensure_ascii=False, indent=2)}

ì´ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê³  ë‹¤ìŒì„ ì„¤ëª…í•˜ì„¸ìš”:
1. ì‹¤í–‰ì´ ì„±ê³µí–ˆëŠ”ì§€ ì—¬ë¶€
2. ì–»ì€ ì •ë³´ë‚˜ ê²°ê³¼ì˜ ì˜ë¯¸
3. ëª©í‘œ ë‹¬ì„±ì— ì–¼ë§ˆë‚˜ ê°€ê¹Œì›Œì¡ŒëŠ”ì§€
4. ë‹¤ìŒì— í•´ì•¼ í•  ì¼ì´ ìˆëŠ”ì§€

í˜„ì¬ ì‘ì—…: {self.current_task['description']}
"""
        
        # LLM í˜¸ì¶œ
        observation = self.llm.generate(user_prompt, system_prompt)
        logger.info(f"ğŸ‘ï¸  LLM ê´€ì°°: {observation[:100]}...")
        
        return observation
    
    def _execute_tool(self, tool_name: str, parameters: Dict[str, Any], step_number: int) -> Dict[str, Any]:
        """ì‹¤ì œ ë„êµ¬ ì‹¤í–‰"""
        try:
            request_data = {
                "action": "execute_tool",
                "tool_name": tool_name,
                "parameters": parameters,
                "request_id": f"llm_agent_step_{step_number}"
            }
            
            request_json = json.dumps(request_data)
            response_json = self.mcp_server.handle_request(request_json)
            response = json.loads(response_json)
            
            result = {
                "success": response["success"],
                "tool_name": tool_name,
                "parameters": parameters,
                "result": response.get("result"),
                "error": response.get("error")
            }
            
            if result["success"]:
                logger.info(f"âœ… ë„êµ¬ ì‹¤í–‰ ì„±ê³µ: {tool_name}")
            else:
                logger.error(f"âŒ ë„êµ¬ ì‹¤í–‰ ì‹¤íŒ¨: {tool_name} - {result['error']}")
            
            return result
            
        except Exception as e:
            logger.error(f"ë„êµ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {
                "success": False,
                "tool_name": tool_name,
                "parameters": parameters,
                "error": str(e)
            }
    
    def _build_context(self, step_number: int) -> str:
        """í˜„ì¬ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
        context = ""
        
        if self.execution_history:
            context += "ì§€ê¸ˆê¹Œì§€ì˜ ì‹¤í–‰ ê¸°ë¡:\n"
            for entry in self.execution_history[-6:]:  # ìµœê·¼ 6ê°œë§Œ
                context += f"- {entry['type']}: {entry['content'][:100]}...\n"
        else:
            context += "ì´ê²ƒì€ ì²« ë²ˆì§¸ ë‹¨ê³„ì…ë‹ˆë‹¤.\n"
        
        return context
    
    def _format_available_tools(self) -> str:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ë“¤ì„ í¬ë§·íŒ…"""
        if not self.available_tools:
            return "ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        tools_text = ""
        for tool in self.available_tools[:8]:  # ìƒìœ„ 8ê°œë§Œ
            tools_text += f"- {tool['name']}: {tool['description']}\n"
        
        return tools_text
    
    def _is_goal_achieved(self, thought: str) -> bool:
        """ëª©í‘œ ë‹¬ì„± ì—¬ë¶€ í™•ì¸ (LLM ì‘ë‹µ ê¸°ë°˜)"""
        # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±
        achievement_keywords = ["ì™„ë£Œ", "ë‹¬ì„±", "ì„±ê³µ", "ë", "ëª©í‘œ", "ì™„ì„±"]
        thought_lower = thought.lower()
        
        return any(keyword in thought_lower for keyword in achievement_keywords)
    
    def _add_to_history(self, entry_type: str, content: str, step_number: int):
        """ì‹¤í–‰ ê¸°ë¡ì— ì¶”ê°€"""
        self.execution_history.append({
            "type": entry_type,
            "content": content,
            "step": step_number,
            "timestamp": time.time()
        })
    
    def _generate_final_summary(self) -> str:
        """ìµœì¢… ìš”ì•½ ìƒì„±"""
        if not self.execution_history:
            return "ì‹¤í–‰ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤."
        
        # ë§ˆì§€ë§‰ ê´€ì°° ë‚´ìš©ì„ ìš”ì•½ìœ¼ë¡œ ì‚¬ìš©
        observations = [h for h in self.execution_history if h["type"] == "observation"]
        if observations:
            return observations[-1]["content"]
        
        return "ì‘ì—…ì„ ìˆ˜í–‰í–ˆì§€ë§Œ ëª…í™•í•œ ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ìš© ì„¤ì •
    config = LLMConfig(
        provider=LLMProvider.SIMULATION,  # ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ
        model="gpt-3.5-turbo"
    )
    
    agent = LLMAgent("í…ŒìŠ¤íŠ¸ LLM Agent", config)
    print(f"LLM Agent ìƒì„± ì™„ë£Œ: {agent.name}")
    print(f"LLM ì œê³µì: {agent.llm.config.provider.value}")